+++
title = "Speakers"
+++

## Speakers (Abstracts Forthcoming)

### Morning session (11:00am--1:00pm)

#### 1. Introduction: Simon Godsill (11:00am--11:10am)

#### 2. Keynote: Guillaume Hennequin (CBL) (11:10am--11:40am)
[Group website](https://cbl.eng.cam.ac.uk/hennequin/)

*The best-laid plans of mice and men...*

My group's research in computational neuroscience is broadly concerned with how brains control behaviour. In many settings, behaviour involves sequences of actions that need careful planning. In this talk, I will present our recent work on planning in recurrent neural networks (Jensen et al., Nat. Neurosci, 2024). I will show that decision-making networks trained with the ability to decide for themselves (i) when to pause and ‘think’, and (ii) what to do with those ‘thoughts’, recapitulate key aspects of human planning in a flexible navigation task. We show that this network-level theory of planning is supported by neural recordings from the hippocampus of rodents. I will strive to make this accessible for the whole of DivF, and to wrap up in 25 min, but then again, the best-laid plans of mice and men...


#### 3. Aliaksandra Shysheya & Cristiana Diaconu (CBL) (11:40am -- 12:00pm)

*On conditional diffusion models for PDE simulations*

Modelling partial differential equations (PDEs) is of crucial importance in science and engineering. Some of the most common tasks include 1) forecasting, where the aim is to predict future states based on an initial one, as well as 2) inverse problems, such as data assimilation (DA), with the goal of reconstructing an aspect of the PDE (i.e. coefficient, initial condition, full trajectory, etc.) given some partial observations of the solution to the PDE. However, most previous numerical and machine learning approaches that target forecasting cannot be applied out-of-the-box for data assimilation.

Recently, diffusion models have emerged as a powerful tool for conditional generation, being able to flexibly incorporate observations without retraining. In this work, we perform a comparative study of score-based diffusion models for forecasting and assimilation of sparse observations. In particular, we focus on diffusion models that are either presented with the conditional information during training, or conditioned after unconditional training. We address the shortcomings of previous work and develop methods that are able to successfully tackle the combination of forecasting and data assimilation, a task commonly encountered in real-world scenarios such as weather modelling.


#### 4. Jeff Wang (Control)  (12:00pm--12:20pm)

*Control of an unknown physical system with a stability guarantee*



#### 5. Carl Ashworth (CBL) (12:20pm--12:40pm)

*The Neural Dynamics of Endogenous Pain Regulation*


#### 6. Florian Fischer (MIL) (12:40pm--1:00pm)

*Simulating User Movements in VR Interaction*



### Afternoon talks (2:00pm--4:00pm)

#### 1. Keynote: Hatice Gunes (2:00pm--2:30pm)
[Group website](https://cambridge-afar.github.io/)

*TBC*


#### 2. John Bronskill (CBL) (2:30pm--2:50pm)

*LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language*

Machine learning practitioners often face significant challenges in formally integrating their prior knowledge and beliefs into predictive models. Moreover, the expertise needed to integrate this prior knowledge into probabilistic modeling typically limits the application of these models to specialists. Our goal is to build a regression model that can process numerical data and make probabilistic predictions at arbitrary locations, guided by natural language text which describes a user's prior knowledge. Large Language Models (LLMs) provide a useful starting point for designing such a tool since they 1) provide an interface where users can incorporate expert insights in natural language and 2) provide an opportunity for leveraging latent problem-relevant knowledge encoded in LLMs that users may not have themselves. We start by exploring strategies for eliciting explicit, coherent numerical predictive distributions from LLMs. We examine these joint predictive distributions, which we call LLM Processes, over arbitrarily-many quantities in settings such as forecasting, multi-dimensional regression, black-box optimization, and image modeling. We investigate the practical details of prompting to elicit coherent predictive distributions, and demonstrate their effectiveness at regression. Finally, we demonstrate the ability to usefully incorporate text into numerical predictions, improving predictive performance and giving quantitative structure that reflects qualitative descriptions.


#### 3. Erez Li (Control) (2:50pm--3:10pm)

*Single-cell dynamic measurements of antimicrobial response*



#### 4. Yaman Kindap (SigProc) (3:10pm--3:30pm)

*Non-Gaussian Stochastic Differential Equation Models for Dynamical Systems*



#### 5. Term Welcome Address: Simon Godsill (3:30pm--4:00pm)

